{
  "DataPreprocessor": {
    "prefix": "DataPreprocessor",
    "body": [
      "import torch",
      "from torch.utils.data import DataLoader, random_split",
      "from torchvision import transforms, datasets",
      "",
      "",
      "class DataPreprocessor:",
      "    def __init__(",
      "        self,",
      "        dataset_dir,",
      "        batch_size=32,",
      "        validation_split=0.2,",
      "        test_split=0.1,",
      "        random_seed=42,",
      "    ):",
      "        self.dataset_dir = dataset_dir",
      "        self.batch_size = batch_size",
      "        self.validation_split = validation_split",
      "        self.test_split = test_split",
      "        self.random_seed = random_seed",
      "",
      "        self.transform = transforms.Compose(",
      "            [",
      "                transforms.RandomResizedCrop(224),",
      "                transforms.RandomHorizontalFlip(),",
      "                transforms.RandomRotation(15),",
      "                transforms.RandomApply([transforms.Resize(256)], p=0.5),",
      "                transforms.ColorJitter(",
      "                    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1",
      "                ),",
      "                transforms.RandomAffine(10, translate=(0.1, 0.1)),",
      "                transforms.ToTensor(),",
      "                transforms.Normalize(",
      "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]",
      "                ),  # Normalize (ImageNet stats)",
      "            ]",
      "        )",
      "",
      "    def load_data(self):",
      "        dataset = datasets.ImageFolder(root=self.dataset_dir, transform=self.transform)",
      "        return dataset",
      "",
      "    def split_data(self, dataset):",
      "        total_size = len(dataset)",
      "        val_size = int(total_size * self.validation_split)",
      "        test_size = int(total_size * self.test_split)",
      "        train_size = total_size - val_size - test_size",
      "",
      "        train_dataset, val_test_dataset = random_split(",
      "            dataset, [train_size, val_size + test_size]",
      "        )",
      "        val_dataset, test_dataset = random_split(",
      "            val_test_dataset, [val_size, test_size]",
      "        )",
      "",
      "        return train_dataset, val_dataset, test_dataset",
      "",
      "    def get_data_loaders(self):",
      "        dataset = self.load_data()",
      "        train_dataset, val_dataset, test_dataset = self.split_data(dataset)",
      "",
      "        train_loader = DataLoader(",
      "            train_dataset, batch_size=self.batch_size, shuffle=True",
      "        )",
      "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)",
      "        test_loader = DataLoader(",
      "            test_dataset, batch_size=self.batch_size, shuffle=False",
      "        )",
      "",
      "        return {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}",
      "",
      "",
      "dataset_directory = \"path_to_data\"",
      "data_processor = DataPreprocessor(dataset_directory, batch_size=64)",
      "data_loaders = data_processor.get_data_loaders()",
      "train_loader = data_loaders[\"train\"]",
      "val_loader = data_loaders[\"val\"]",
      "test_loader = data_loaders[\"test\"]"
    ],
    "description": "Data Preprocessor for images"
  },
  "FeedforwardNN": {
    "prefix": "FNN",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "",
      "",
      "class FeedforwardNN(nn.Module):",
      "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_prob=0.5):",
      "        \"\"\"",
      "        Args:",
      "            input_dim (int): Dimension of the input layer.",
      "            hidden_dims (list): List of integers representing the number of units in each hidden layer.",
      "            output_dim (int): Dimension of the output layer.",
      "            dropout_prob (float): Probability of dropping a unit during dropout.",
      "        \"\"\"",
      "        super(FeedforwardNN, self).__init__()",
      "",
      "        layers = []",
      "        prev_dim = input_dim",
      "",
      "        # Create hidden layers",
      "        for h_dim in hidden_dims:",
      "            layers.append(nn.Linear(prev_dim, h_dim))",
      "            layers.append(nn.BatchNorm1d(h_dim))",
      "            layers.append(nn.ReLU())",
      "            layers.append(nn.Dropout(dropout_prob))",
      "            prev_dim = h_dim",
      "",
      "        # Output layer",
      "        layers.append(nn.Linear(prev_dim, output_dim))",
      "",
      "        # Combine layers into a sequential model",
      "        self.network = nn.Sequential(*layers)",
      "",
      "        self._initialize_weights()",
      "",
      "    def _initialize_weights(self):",
      "        for m in self.modules():",
      "            if isinstance(m, nn.Linear):",
      "                nn.init.xavier_uniform_(m.weight)",
      "                if m.bias is not None:",
      "                    nn.init.zeros_(m.bias)",
      "",
      "    def forward(self, x):",
      "        x = self.network(x)",
      "        return F.softmax(x, dim=1)",
      "",
      "",
      "input_dim = 784",
      "hidden_dims = [512, 256, 128]",
      "output_dim = 10",
      "dropout_prob = 0.5",
      "",
      "model = FeedforwardNN(input_dim, hidden_dims, output_dim, dropout_prob)",
      "x = torch.randn(16, input_dim)",
      "output = model(x)"
    ],
    "description": "Feedforward Neural Network"
  },
  "CNN": {
    "prefix": "CNN",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "",
      "",
      "class CNN(nn.Module):",
      "    def __init__(self, input_channels=3, num_classes=10):",
      "        \"\"\"",
      "        Args:",
      "            input_channels (int): Number of input channels (e.g., 3 for RGB images).",
      "            num_classes (int): Number of output classes (e.g., 10 for CIFAR-10).",
      "        \"\"\"",
      "        super(CNN, self).__init__()",
      "",
      "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)",
      "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)",
      "",
      "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)",
      "",
      "        self.fc1 = nn.Linear(64 * 8 * 8, 512)",
      "        self.fc2 = nn.Linear(512, num_classes)",
      "",
      "    def forward(self, x):",
      "        \"\"\"",
      "        Args:",
      "            x (torch.Tensor): Input tensor of shape (batch_size, input_channels, height, width).",
      "",
      "        Returns:",
      "            torch.Tensor: Output tensor of shape (batch_size, num_classes).",
      "        \"\"\"",
      "        x = self.pool(F.relu(self.conv1(x)))",
      "        x = self.pool(F.relu(self.conv2(x)))",
      "",
      "        x = x.view(-1, 64 * 8 * 8)  # Flatten to (batch_size, 64 * 8 * 8)",
      "",
      "        x = F.relu(self.fc1(x))",
      "        x = self.fc2(x)",
      "",
      "        return x",
      "",
      "",
      "model = CNN(input_channels=3, num_classes=10)",
      "sample_input = torch.randn(4, 3, 32, 32)",
      "output = model(sample_input)"
    ],
    "description": "Convolution Neural Network"
  },
  "PretrainedResNet": {
    "prefix": "PretrainedResNet",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "import torchvision.models as models",
      "",
      "",
      "class PretrainedCNN(nn.Module):",
      "    def __init__(self, num_classes=10, pretrained=True, freeze_layers=True):",
      "        \"\"\"",
      "        Args:",
      "            architecture (str): The architecture to load (e.g., 'resnet18').",
      "            num_classes (int): Number of output classes (e.g., 10 for CIFAR-10).",
      "            pretrained (bool): If True, load weights from a pretrained model.",
      "        \"\"\"",
      "        super(PretrainedCNN, self).__init__()",
      "",
      "        self.model = models.resnet18(weights=\"IMAGENET1K_V2\" if pretrained else None)",
      "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)",
      "",
      "        if freeze_layers:",
      "            for param in self.model.parameters():",
      "                param.requires_grad = False",
      "            for param in self.model.fc.parameters():",
      "                param.requires_grad = True",
      "",
      "    def forward(self, x):",
      "        return self.model(x)",
      "",
      "",
      "model = PretrainedCNN(num_classes=10)",
      "sample_input = torch.randn(4, 3, 32, 32)",
      "output = model(sample_input)"
    ],
    "description": "Pretrained ResNet model"
  },
  "ResNet": {
  "prefix": "ResNet",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "",
      "",
      "class BasicBlock(nn.Module):",
      "    \"\"\"Basic residual block used in ResNet.\"\"\"",
      "",
      "    def __init__(self, in_channels, out_channels, stride=1):",
      "        super(BasicBlock, self).__init__()",
      "        self.conv1 = nn.Conv2d(",
      "            in_channels,",
      "            out_channels,",
      "            kernel_size=3,",
      "            stride=stride,",
      "            padding=1,",
      "            bias=False,",
      "        )",
      "        self.bn1 = nn.BatchNorm2d(out_channels)",
      "        self.relu = nn.ReLU(inplace=True)",
      "        self.conv2 = nn.Conv2d(",
      "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False",
      "        )",
      "        self.bn2 = nn.BatchNorm2d(out_channels)",
      "",
      "        self.shortcut = nn.Sequential()",
      "        if stride != 1 or in_channels != out_channels:",
      "            self.shortcut = nn.Sequential(",
      "                nn.Conv2d(",
      "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False",
      "                ),",
      "                nn.BatchNorm2d(out_channels),",
      "            )",
      "",
      "    def forward(self, x):",
      "        return self.relu(",
      "            self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))",
      "        ) + self.shortcut(x)",
      "",
      "",
      "class ResNet(nn.Module):",
      "    def __init__(self, block, layers, num_classes=10):",
      "        \"\"\"",
      "        Args:",
      "            block (nn.Module): The residual block (e.g., BasicBlock).",
      "            layers (list): List containing the number of blocks for each layer.",
      "            num_classes (int): Number of output classes.",
      "        \"\"\"",
      "        super(ResNet, self).__init__()",
      "        self.in_channels = 64",
      "",
      "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)",
      "        self.bn1 = nn.BatchNorm2d(64)",
      "        self.relu = nn.ReLU(inplace=True)",
      "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)",
      "",
      "        # Layer groups",
      "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)",
      "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)",
      "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)",
      "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)",
      "",
      "        # Fully connected layers",
      "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))",
      "        self.fc = nn.Linear(512, num_classes)",
      "",
      "    def _make_layer(self, block, out_channels, num_blocks, stride):",
      "        \"\"\"Creates a sequence of residual blocks.\"\"\"",
      "        layers = [block(self.in_channels, out_channels, stride)]",
      "        self.in_channels = out_channels",
      "        for _ in range(1, num_blocks):",
      "            layers.append(block(self.in_channels, out_channels))",
      "        return nn.Sequential(*layers)",
      "",
      "    def forward(self, x):",
      "        x = self.relu(self.bn1(self.conv1(x)))",
      "        x = self.maxpool(x)",
      "",
      "        x = self.layer1(x)",
      "        x = self.layer2(x)",
      "        x = self.layer3(x)",
      "        x = self.layer4(x)",
      "",
      "        x = self.avgpool(x)",
      "        x = torch.flatten(x, 1)",
      "        x = self.fc(x)",
      "        return x",
      "",
      "",
      "layers = [2, 2, 2, 2]  # Number of blocks in each of the 4 layers",
      "model = ResNet(BasicBlock, layers, num_classes=10)"
    ],
    "description": "Basic ResNet model from scratch"
  },
  "RNN": {
  "prefix": "RNN",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "",
      "",
      "class RNN(nn.Module):",
      "    \"\"\"",
      "    Args:",
      "        input_size (int): Number of input features (e.g., vocabulary size, feature dimension).",
      "        hidden_size (int): Number of features in the hidden state.",
      "        output_size (int): Number of output classes (or output features).",
      "        num_layers (int): Number of recurrent layers.",
      "        dropout (float): Dropout probability for regularization.",
      "    \"\"\"",
      "",
      "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0):",
      "        super(RNN, self).__init__()",
      "        self.hidden_size = hidden_size",
      "        self.num_layers = num_layers",
      "",
      "        self.rnn = nn.RNN(",
      "            input_size,",
      "            hidden_size,",
      "            num_layers,",
      "            batch_first=True,",
      "            dropout=dropout,",
      "            bidirectional=False,",
      "        )",
      "",
      "        self.fc = nn.Linear(hidden_size, output_size)",
      "",
      "    def forward(self, x):",
      "        \"\"\"",
      "        Args:",
      "            x (Tensor): Input tensor of shape (batch_size, sequence_length, input_size).",
      "        Returns:",
      "            Tensor: Output predictions of shape (batch_size, output_size).",
      "        \"\"\"",
      "        # Initialize hidden state (h_0)",
      "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)",
      "",
      "        # Forward pass through RNN",
      "        out, _ = self.rnn(x, h0)",
      "",
      "        out = out[:, -1, :]",
      "",
      "        out = self.fc(out)",
      "        return out",
      "",
      "",
      "model = RNN(input_size=10, hidden_size=50, output_size=1)",
      ""
    ],
    "description": "Basic Recurrent Neural Network"
  },
  "LSTM": {
  "prefix": "LSTM",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "",
      "",
      "class LSTM(nn.Module):",
      "    \"\"\"",
      "    Args:",
      "        input_size (int): Number of input features per timestep.",
      "        hidden_size (int): Number of features in the hidden state.",
      "        num_layers (int): Number of LSTM layers.",
      "        output_size (int): Number of output features (e.g., for classification or regression).",
      "        dropout (float): Dropout probability to prevent overfitting.",
      "    \"\"\"",
      "",
      "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):",
      "        super(LSTM, self).__init__()",
      "",
      "        self.lstm = nn.LSTM(",
      "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout",
      "        )",
      "",
      "        self.fc = nn.Linear(hidden_size, output_size)",
      "",
      "    def forward(self, x):",
      "        \"\"\"",
      "        Args:",
      "            x (Tensor): Input tensor of shape (batch_size, seq_len, input_size).",
      "",
      "        Returns:",
      "            Tensor: Output tensor of shape (batch_size, output_size).",
      "        \"\"\"",
      "        lstm_out, (h_n, c_n) = self.lstm(x)",
      "        # Take the output from the last time step",
      "        last_hidden_state = lstm_out[:, -1, :]  # (batch_size, hidden_size)",
      "",
      "        out = self.fc(last_hidden_state)",
      "        return out",
      "",
      "",
      "input_size = 10",
      "model = LSTM(input_size, hidden_size=50, num_layers=2, output_size=1, dropout=0.3)",
      "x = torch.randn(32, 10, input_size)  # (batch_size, seq_len, input_size)",
      "output = model(x)"
    ],
    "description": "Basic Long Short-Term Memory Model"
  },
  "Self-Attention": {
    "prefix": "SelfAttention",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "",
      "",
      "class SelfAttention(nn.Module):",
      "    \"\"\"",
      "    Attributes:",
      "        embed_size (int): Dimension of input embeddings.",
      "        heads (int): Number of attention heads.",
      "    \"\"\"",
      "",
      "    def __init__(self, embed_size: int, heads: int):",
      "        super(SelfAttention, self).__init__()",
      "",
      "        self.embed_size = embed_size",
      "        self.heads = heads",
      "        self.head_dim = embed_size // heads",
      "",
      "        assert self.head_dim * heads == embed_size, (",
      "            \"Embedding size must be divisible by number of heads.\"",
      "        )",
      "",
      "        # Linear layers to project input to queries, keys, and values",
      "        self.queries = nn.Linear(embed_size, embed_size)",
      "        self.keys = nn.Linear(embed_size, embed_size)",
      "        self.values = nn.Linear(embed_size, embed_size)",
      "        self.fc_out = nn.Linear(embed_size, embed_size)",
      "",
      "    def forward(",
      "        self,",
      "        values: torch.Tensor,",
      "        keys: torch.Tensor,",
      "        query: torch.Tensor,",
      "        mask: torch.Tensor = None,",
      "    ) -> torch.Tensor:",
      "        \"\"\"",
      "        Args:",
      "            values (Tensor): Values tensor (batch_size, seq_len, embed_size)",
      "            keys (Tensor): Keys tensor (batch_size, seq_len, embed_size)",
      "            query (Tensor): Query tensor (batch_size, seq_len, embed_size)",
      "            mask (Tensor, optional): Mask tensor to avoid attention to certain positions (batch_size, seq_len). Default is None.",
      "",
      "        Returns:",
      "            Tensor: The result of attention applied to values (batch_size, seq_len, embed_size).",
      "        \"\"\"",
      "        batch_size = query.shape[0]",
      "",
      "        # Linear projections",
      "        values = self.values(values)",
      "        keys = self.keys(keys)",
      "        queries = self.queries(query)",
      "",
      "        # Split into multiple heads",
      "        values = values.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)",
      "        keys = keys.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)",
      "        queries = queries.view(batch_size, -1, self.heads, self.head_dim).transpose(",
      "            1, 2",
      "        )",
      "",
      "        # Scaled dot-product attention",
      "        energy = torch.einsum(",
      "            \"bhqd,bhkd->bhqk\", [queries, keys]",
      "        )  # (batch_size, heads, query_len, key_len)",
      "",
      "        if mask is not None:",
      "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))  # Mask out padding",
      "",
      "        attention = torch.nn.functional.softmax(energy / (self.head_dim**0.5), dim=-1)",
      "",
      "        out = torch.einsum(",
      "            \"bhqk,bhvd->bhqd\", [attention, values]",
      "        )  # (batch_size, heads, query_len, value_dim)",
      "",
      "        out = (",
      "            out.transpose(1, 2)",
      "            .contiguous()",
      "            .view(batch_size, -1, self.heads * self.head_dim)",
      "        )  # Concat heads",
      "",
      "        out = self.fc_out(out)",
      "        return out",
      "",
      "",
      "attention_layer = SelfAttention(embed_size=256, heads=8)",
      "values = torch.rand(64, 10, 256)  # batch_size=64, seq_len=10, embed_size=256",
      "keys = values",
      "query = values",
      "mask = None",
      "output = attention_layer(values, keys, query, mask)",
      ""
    ],
    "description": "Multi-head Self-Attention"
  },
  "Softmax": {
    "prefix": "",
    "body": [
      "def softmax(x, dim=-1):",
      "    x_max, _ = torch.max(x, dim=dim, keepdim=True)",
      "    x_stable = x - x_max",
      "",
      "    exp_x = torch.exp(x_stable)",
      "    sum_exp = torch.sum(exp_x, dim=dim, keepdim=True)",
      "    return exp_x / sum_exp"
    ],
    "description": "Numerically stable softmax implementation"
  },
  "UNet": {
    "prefix": "UNet",
    "body": [
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "",
      "",
      "class DoubleConv(nn.Module):",
      "    def __init__(self, in_channels, out_channels, mid_channels=None):",
      "        super().__init__()",
      "        if not mid_channels:",
      "            mid_channels = out_channels",
      "",
      "        self.double_conv = nn.Sequential(",
      "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),",
      "            nn.BatchNorm2d(mid_channels),",
      "            nn.ReLU(inplace=True),",
      "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),",
      "            nn.BatchNorm2d(out_channels),",
      "            nn.ReLU(inplace=True),",
      "        )",
      "",
      "    def forward(self, x):",
      "        return self.double_conv(x)",
      "",
      "",
      "class Down(nn.Module):",
      "    def __init__(self, in_channels, out_channels):",
      "        super().__init__()",
      "        self.maxpool_conv = nn.Sequential(",
      "            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)",
      "        )",
      "",
      "    def forward(self, x):",
      "        return self.maxpool_conv(x)",
      "",
      "",
      "class Up(nn.Module):",
      "    def __init__(self, in_channels, out_channels, bilinear=True):",
      "        super().__init__()",
      "",
      "        if bilinear:",
      "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)",
      "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)",
      "        else:",
      "            self.up = nn.ConvTranspose2d(",
      "                in_channels, in_channels // 2, kernel_size=2, stride=2",
      "            )",
      "            self.conv = DoubleConv(in_channels, out_channels)",
      "",
      "    def forward(self, x1, x2):",
      "        # x1 = upsampled feature map",
      "        # x2 = skip connection",
      "        x1 = self.up(x1)",
      "",
      "        # Handle odd-shaped tensors",
      "        diffY = x2.size()[2] - x1.size()[2]",
      "        diffX = x2.size()[3] - x1.size()[3]",
      "",
      "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])",
      "",
      "        x = torch.cat([x2, x1], dim=1)",
      "        return self.conv(x)",
      "",
      "",
      "class OutConv(nn.Module):",
      "    def __init__(self, in_channels, out_channels):",
      "        super().__init__()",
      "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)",
      "",
      "    def forward(self, x):",
      "        return self.conv(x)",
      "",
      "",
      "class UNet(nn.Module):",
      "    def __init__(self, n_channels, n_classes, bilinear=True):",
      "        super().__init__()",
      "",
      "        self.n_channels = n_channels",
      "        self.n_classes = n_classes",
      "        self.bilinear = bilinear",
      "",
      "        self.inc = DoubleConv(n_channels, 64)",
      "        self.down1 = Down(64, 128)",
      "        self.down2 = Down(128, 256)",
      "        self.down3 = Down(256, 512)",
      "",
      "        factor = 2 if bilinear else 1",
      "        self.down4 = Down(512, 1024 // factor)",
      "",
      "        self.up1 = Up(1024, 512 // factor, bilinear)",
      "        self.up2 = Up(512, 256 // factor, bilinear)",
      "        self.up3 = Up(256, 128 // factor, bilinear)",
      "        self.up4 = Up(128, 64, bilinear)",
      "",
      "        self.outc = OutConv(64, n_classes)",
      "",
      "    def forward(self, x):",
      "        x1 = self.inc(x)",
      "        x2 = self.down1(x1)",
      "        x3 = self.down2(x2)",
      "        x4 = self.down3(x3)",
      "        x5 = self.down4(x4)",
      "",
      "        x = self.up1(x5, x4)",
      "        x = self.up2(x, x3)",
      "        x = self.up3(x, x2)",
      "        x = self.up4(x, x1)",
      "",
      "        logits = self.outc(x)",
      "        return logits",
      "",
      "",
      "model = UNet(n_channels=3, n_classes=1)",
      "x = torch.randn(1, 3, 256, 256)",
      "y = model(x)"
    ],
    "description": "U-Net implementation"
  }
}
